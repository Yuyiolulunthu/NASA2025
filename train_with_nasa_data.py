
import numpy as np
import pandas as pd
import joblib
import json
from pathlib import Path
from datetime import datetime

from sklearn.model_selection import GroupKFold, cross_val_predict
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.calibration import CalibratedClassifierCV

from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.ensemble import RandomForestClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression

import warnings
warnings.filterwarnings('ignore')


class NASAExoplanetTrainer:
    """NASA çœŸå¯¦è³‡æ–™è¨“ç·´å™¨"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.selected_features = None
        self.model = None
        
        Path("models").mkdir(exist_ok=True)
    
    def load_kepler_data(self, filepath='kepler_data.csv'):
        """è¼‰å…¥ Kepler è³‡æ–™"""
        print(f"ğŸ“¥ è¼‰å…¥ Kepler è³‡æ–™: {filepath}")
        
        df = pd.read_csv(filepath)
        print(f"âœ“ åŸå§‹è³‡æ–™: {df.shape}")
        
        # æ¨™æº–åŒ–æ¨™ç±¤åç¨±
        label_mapping = {
            'CONFIRMED': 'CONFIRMED',
            'CANDIDATE': 'CANDIDATE',
            'FALSE POSITIVE': 'FALSE POSITIVE',
            'NOT DISPOSITIONED': None  # ç§»é™¤æœªåˆ†é¡çš„
        }
        
        df['disposition'] = df['koi_disposition'].map(label_mapping)
        df = df.dropna(subset=['disposition'])
        
        print(f"âœ“ éæ¿¾å¾Œè³‡æ–™: {df.shape}")
        print(f"\né¡åˆ¥åˆ†ä½ˆ:")
        print(df['disposition'].value_counts())
        
        return df
    
    def engineer_features(self, df):
        """ç‰¹å¾µå·¥ç¨‹ - ä½¿ç”¨ Kepler çœŸå¯¦æ¬„ä½"""
        print("\nğŸ”§ ç‰¹å¾µå·¥ç¨‹...")
        
        # Kepler çš„ä¸»è¦ç‰¹å¾µæ¬„ä½
        feature_columns = [
            'koi_period',       # è»Œé“é€±æœŸ
            'koi_duration',     # å‡Œæ—¥æŒçºŒæ™‚é–“
            'koi_depth',        # å‡Œæ—¥æ·±åº¦
            'koi_prad',         # è¡Œæ˜ŸåŠå¾‘
            'koi_teq',          # å¹³è¡¡æº«åº¦
            'koi_insol',        # æ†æ˜Ÿè¼»å°„
            'koi_model_snr',    # ä¿¡å™ªæ¯”
            'koi_steff',        # æ†æ˜Ÿæº«åº¦
            'koi_srad',         # æ†æ˜ŸåŠå¾‘
            'koi_slogg',        # æ†æ˜Ÿè¡¨é¢é‡åŠ›
        ]
        
        # åªä¿ç•™å­˜åœ¨çš„æ¬„ä½
        available_features = [col for col in feature_columns if col in df.columns]
        
        X = df[available_features].copy()
        
        # è™•ç†ç¼ºå¤±å€¼
        print(f"ç¼ºå¤±å€¼è™•ç†å‰: {X.shape}")
        
        # ç”¨ä¸­ä½æ•¸å¡«è£œæ•¸å€¼å‹æ¬„ä½
        for col in X.columns:
            if X[col].isnull().any():
                median_val = X[col].median()
                X[col].fillna(median_val, inplace=True)
                print(f"  å¡«è£œ {col}: {X[col].isnull().sum()} å€‹ç¼ºå¤±å€¼")
        
        # è¡ç”Ÿç‰¹å¾µ
        if 'koi_period' in X.columns and 'koi_duration' in X.columns:
            X['duration_period_ratio'] = X['koi_duration'] / (X['koi_period'] * 24 + 1e-6)
        
        if 'koi_depth' in X.columns and 'koi_prad' in X.columns:
            X['depth_radius_ratio'] = X['koi_depth'] / (X['koi_prad'] ** 2 + 1e-6)
        
        if 'koi_model_snr' in X.columns:
            X['log_snr'] = np.log1p(X['koi_model_snr'])
        
        if 'koi_teq' in X.columns and 'koi_insol' in X.columns:
            X['temp_insol_ratio'] = X['koi_teq'] / (X['koi_insol'] + 1e-6)
        
        # ç§»é™¤ç„¡é™å€¼å’Œæ¥µç«¯å€¼
        X = X.replace([np.inf, -np.inf], np.nan)
        X = X.fillna(X.median())
        
        print(f"âœ“ ç‰¹å¾µå·¥ç¨‹å®Œæˆ: {X.shape[1]} å€‹ç‰¹å¾µ")
        print(f"ç‰¹å¾µåˆ—è¡¨: {X.columns.tolist()}")
        
        return X
    
    def select_features(self, X, y, threshold=50):
        """ç‰¹å¾µé¸æ“‡"""
        print(f"\nğŸ¯ ç‰¹å¾µé¸æ“‡ (Top {threshold})...")
        
        lgbm = LGBMClassifier(
            n_estimators=100,
            random_state=42,
            class_weight='balanced',
            verbose=-1
        )
        lgbm.fit(X, y)
        
        importances = pd.Series(
            lgbm.feature_importances_,
            index=X.columns
        ).sort_values(ascending=False)
        
        # é¸æ“‡ Top N æˆ–æ‰€æœ‰ç‰¹å¾µï¼ˆå¦‚æœç‰¹å¾µæ•¸ < thresholdï¼‰
        n_select = min(threshold, len(X.columns))
        self.selected_features = importances.head(n_select).index.tolist()
        
        print(f"âœ“ é¸æ“‡äº† {len(self.selected_features)} å€‹ç‰¹å¾µ")
        print(f"Top 10 é‡è¦ç‰¹å¾µ:")
        for i, (feat, imp) in enumerate(importances.head(10).items(), 1):
            print(f"  {i}. {feat}: {imp:.4f}")
        
        return X[self.selected_features]
    
    def build_model(self):
        """æ§‹å»ºå †ç–Šæ¨¡å‹"""
        print("\nğŸ—ï¸  æ§‹å»ºå †ç–Šæ¨¡å‹...")
        
        base_learners = [
            ('lgbm', LGBMClassifier(
                n_estimators=200,
                learning_rate=0.05,
                max_depth=5,
                class_weight='balanced',
                random_state=42,
                verbose=-1
            )),
            ('xgb', XGBClassifier(
                n_estimators=200,
                learning_rate=0.05,
                max_depth=5,
                random_state=42,
                eval_metric='logloss',
                verbosity=0
            )),
            ('catboost', CatBoostClassifier(
                iterations=200,
                learning_rate=0.05,
                depth=5,
                random_state=42,
                verbose=False
            )),
            ('rf', RandomForestClassifier(
                n_estimators=200,
                max_depth=10,
                class_weight='balanced',
                random_state=42,
                n_jobs=-1
            ))
        ]
        
        meta_learner = LogisticRegression(
            max_iter=1000,
            class_weight='balanced',
            random_state=42
        )
        
        stacking_model = StackingClassifier(
            estimators=base_learners,
            final_estimator=meta_learner,
            cv=3,
            n_jobs=-1
        )
        
        print("âœ“ æ¨¡å‹æ§‹å»ºå®Œæˆ")
        return stacking_model
    
    def train(self, X, y, groups):
        """è¨“ç·´æ¨¡å‹"""
        print("\n" + "="*60)
        print("ğŸš€ é–‹å§‹è¨“ç·´")
        print("="*60)
        
        # æ¨™æº–åŒ–
        X_scaled = self.scaler.fit_transform(X)
        X_scaled = pd.DataFrame(X_scaled, columns=X.columns)
        
        # ç·¨ç¢¼æ¨™ç±¤
        y_encoded = self.label_encoder.fit_transform(y)
        print(f"\né¡åˆ¥å°æ‡‰:")
        for i, label in enumerate(self.label_encoder.classes_):
            print(f"  {i}: {label}")
        
        # GroupKFold
        gkf = GroupKFold(n_splits=5)
        
        # æ§‹å»ºæ¨¡å‹
        stacking_model = self.build_model()
        
        # äº¤å‰é©—è­‰
        print(f"\nğŸ“Š 5-Fold äº¤å‰é©—è­‰...")
        oof_preds = cross_val_predict(
            stacking_model, X_scaled, y_encoded,
            cv=gkf.split(X_scaled, y_encoded, groups),
            method='predict_proba',
            n_jobs=-1,
            verbose=1
        )
        
        # å®Œæ•´è¨“ç·´
        print("\nğŸ¯ å®Œæ•´è³‡æ–™è¨“ç·´...")
        stacking_model.fit(X_scaled, y_encoded)
        
        # æ©Ÿç‡æ ¡æº–
        print("\nâš–ï¸  æ©Ÿç‡æ ¡æº–...")
        calibrated_model = CalibratedClassifierCV(
            stacking_model,
            method='isotonic',
            cv=3
        )
        calibrated_model.fit(X_scaled, y_encoded)
        
        self.model = calibrated_model
        
        # è©•ä¼°
        self._evaluate(y_encoded, oof_preds)
        
        print("\nâœ… è¨“ç·´å®Œæˆï¼")
        return self
    
    def _evaluate(self, y_true, y_pred_proba):
        """è©•ä¼°æ¨¡å‹"""
        print("\n" + "="*60)
        print("ğŸ“ˆ æ¨¡å‹è©•ä¼°")
        print("="*60)
        
        y_pred = np.argmax(y_pred_proba, axis=1)
        
        print("\nåˆ†é¡å ±å‘Š:")
        print(classification_report(
            y_true, y_pred,
            target_names=self.label_encoder.classes_
        ))
        
        print("\næ··æ·†çŸ©é™£:")
        cm = confusion_matrix(y_true, y_pred)
        print(cm)
        
        # è¨ˆç®—æ¯å€‹é¡åˆ¥çš„æº–ç¢ºç‡
        print("\nå„é¡åˆ¥æº–ç¢ºç‡:")
        for i, label in enumerate(self.label_encoder.classes_):
            if cm[i].sum() > 0:
                acc = cm[i, i] / cm[i].sum()
                print(f"  {label}: {acc:.2%}")
        
        try:
            roc_auc = roc_auc_score(
                y_true, y_pred_proba,
                multi_class='ovr',
                average='weighted'
            )
            print(f"\nWeighted ROC-AUC: {roc_auc:.4f}")
        except:
            pass
    
    def save(self, version=None):
        """å„²å­˜æ¨¡å‹"""
        version = version or datetime.now().strftime("%Y%m%d_%H%M%S")
        
        save_dict = {
            'model': self.model,
            'scaler': self.scaler,
            'label_encoder': self.label_encoder,
            'selected_features': self.selected_features,
            'version': version,
            'timestamp': datetime.now().isoformat(),
            'data_source': 'NASA Kepler'
        }
        
        model_path = Path("models") / f"exoplanet_model_{version}.pkl"
        joblib.dump(save_dict, model_path)
        
        feature_path = Path("models") / "feature_list.json"
        with open(feature_path, 'w') as f:
            json.dump({
                'features': self.selected_features,
                'version': version,
                'data_source': 'NASA Kepler'
            }, f, indent=2)
        
        print(f"\nğŸ’¾ æ¨¡å‹å·²å„²å­˜: {model_path}")
        print(f"ğŸ’¾ ç‰¹å¾µåˆ—è¡¨: {feature_path}")
        
        return model_path


def main():
    """ä¸»è¨“ç·´æµç¨‹"""
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘     NASA çœŸå¯¦è³‡æ–™è¨“ç·´ç³»çµ±                              â•‘
    â•‘     NASA Exoplanet Real Data Training                 â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # æª¢æŸ¥è³‡æ–™æª”æ¡ˆ
    data_file = 'kepler_data.csv'
    if not Path(data_file).exists():
        print(f"âŒ æ‰¾ä¸åˆ° {data_file}")
        print("\nè«‹å…ˆä¸‹è¼‰ NASA è³‡æ–™ï¼š")
        print("1. åŸ·è¡Œ: python download_nasa_data.py")
        print("2. æˆ–æ‰‹å‹•ä¸‹è¼‰: https://exoplanetarchive.ipac.caltech.edu/")
        return
    
    # åˆå§‹åŒ–è¨“ç·´å™¨
    trainer = NASAExoplanetTrainer()
    
    # è¼‰å…¥è³‡æ–™
    df = trainer.load_kepler_data(data_file)
    
    # ç‰¹å¾µå·¥ç¨‹
    X = trainer.engineer_features(df)
    y = df['disposition']
    groups = df['kepid'] if 'kepid' in df.columns else np.arange(len(df))
    
    # ç‰¹å¾µé¸æ“‡
    X_selected = trainer.select_features(X, y, threshold=50)
    
    # è¨“ç·´
    trainer.train(X_selected, y, groups)
    
    # å„²å­˜
    model_path = trainer.save()
    
    print("\n" + "="*60)
    print("ğŸ‰ è¨“ç·´å®Œæˆï¼")
    print("="*60)
    print("\næ¨¡å‹è³‡è¨Š:")
    print(f"  - è¨“ç·´è³‡æ–™: {len(df):,} ç­†")
    print(f"  - ç‰¹å¾µæ•¸: {len(trainer.selected_features)}")
    print(f"  - é¡åˆ¥æ•¸: {len(trainer.label_encoder.classes_)}")
    print(f"  - æ¨¡å‹æª”æ¡ˆ: {model_path}")
    print("\nä¸‹ä¸€æ­¥:")
    print("1. å•Ÿå‹• API: python backend/app.py")
    print("2. å•Ÿå‹•å‰ç«¯: streamlit run frontend/simple_app.py")


if __name__ == "__main__":
    main()