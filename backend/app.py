"""
ç³»å¤–è¡Œæ˜Ÿæª¢æ¸¬ API - FastAPI Backend
æä¾›é æ¸¬ã€è§£é‡‹ã€æ‰¹æ¬¡è™•ç†ç­‰åŠŸèƒ½
"""

from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field, validator
from typing import Optional, List, Dict
import pandas as pd
import numpy as np
import joblib
import json
import shap
from pathlib import Path
from datetime import datetime
import io

app = FastAPI(
    title="Exoplanet Detection API",
    description="NASA ç³»å¤–è¡Œæ˜Ÿæª¢æ¸¬ AI æ¨¡å‹ API",
    version="1.0.0"
)

# CORS è¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨åŸŸè®Šæ•¸
MODEL = None
SCALER = None
LABEL_ENCODER = None
FEATURES = None
EXPLAINER = None


class FeatureInput(BaseModel):
    """å–®ç­†ç‰¹å¾µè¼¸å…¥"""
    koi_period: float = Field(..., description="è»Œé“é€±æœŸ (å¤©)")
    koi_duration: float = Field(..., description="å‡Œæ—¥æŒçºŒæ™‚é–“ (å°æ™‚)")
    koi_depth: float = Field(..., description="å‡Œæ—¥æ·±åº¦ (ppm)")
    koi_prad: Optional[float] = Field(None, description="è¡Œæ˜ŸåŠå¾‘ (åœ°çƒåŠå¾‘)")
    koi_teq: Optional[float] = Field(None, description="å¹³è¡¡æº«åº¦ (K)")
    koi_insol: Optional[float] = Field(None, description="æ†æ˜Ÿè¼»å°„ (åœ°çƒ)")
    koi_model_snr: Optional[float] = Field(None, description="ä¿¡å™ªæ¯”")
    koi_steff: Optional[float] = Field(None, description="æ†æ˜Ÿæœ‰æ•ˆæº«åº¦ (K)")
    koi_srad: Optional[float] = Field(None, description="æ†æ˜ŸåŠå¾‘ (å¤ªé™½åŠå¾‘)")
    
    @validator('koi_period')
    def validate_period(cls, v):
        if v <= 0:
            raise ValueError('è»Œé“é€±æœŸå¿…é ˆå¤§æ–¼ 0')
        return v
    
    @validator('koi_depth')
    def validate_depth(cls, v):
        if v < 0:
            raise ValueError('å‡Œæ—¥æ·±åº¦ä¸èƒ½ç‚ºè² æ•¸')
        return v


class PredictionResponse(BaseModel):
    """é æ¸¬çµæœ"""
    label: str
    confidence: float
    probabilities: Dict[str, float]
    timestamp: str


class BatchPredictionResponse(BaseModel):
    """æ‰¹æ¬¡é æ¸¬çµæœ"""
    total: int
    predictions: List[Dict]
    summary: Dict[str, int]
    timestamp: str


class ExplainResponse(BaseModel):
    """è§£é‡‹çµæœ"""
    prediction: str
    top_features: List[Dict[str, float]]
    shap_values: List[float]


@app.on_event("startup")
async def load_model():
    """å•Ÿå‹•æ™‚è¼‰å…¥æ¨¡å‹"""
    global MODEL, SCALER, LABEL_ENCODER, FEATURES, EXPLAINER
    
    models_dir = Path("models")
    
    try:
        # å°‹æ‰¾æœ€æ–°çš„æ¨¡å‹
        model_files = list(models_dir.glob("exoplanet_model_*.pkl"))
        if not model_files:
            print("âš ï¸  è­¦å‘Š: æ‰¾ä¸åˆ°è¨“ç·´å¥½çš„æ¨¡å‹")
            print("è«‹å…ˆåŸ·è¡Œ: python train_exoplanet_model.py")
            return
        
        latest_model = max(model_files, key=lambda p: p.stat().st_mtime)
        
        print(f"ğŸ“¦ è¼‰å…¥æ¨¡å‹: {latest_model}")
        model_dict = joblib.load(latest_model)
        
        MODEL = model_dict['model']
        SCALER = model_dict['scaler']
        LABEL_ENCODER = model_dict['label_encoder']
        FEATURES = model_dict['selected_features']
        
        print(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        print(f"   - ç‰ˆæœ¬: {model_dict.get('version', 'unknown')}")
        print(f"   - ç‰¹å¾µæ•¸: {len(FEATURES)}")
        print(f"   - é¡åˆ¥: {LABEL_ENCODER.classes_.tolist()}")
        
        # åˆå§‹åŒ– SHAP explainerï¼ˆå¯é¸ï¼‰
        try:
            # ä½¿ç”¨å°æ¨£æœ¬åˆå§‹åŒ–
            X_sample = pd.DataFrame(
                SCALER.mean_.reshape(1, -1),
                columns=FEATURES
            )
            EXPLAINER = shap.Explainer(MODEL.predict, X_sample)
            print("âœ… SHAP explainer åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸  SHAP explainer åˆå§‹åŒ–å¤±æ•—: {e}")
            EXPLAINER = None
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        raise


def prepare_features(data: Dict) -> pd.DataFrame:
    """æº–å‚™ç‰¹å¾µ"""
    # å»ºç«‹å®Œæ•´ç‰¹å¾µå­—å…¸ï¼ˆå«è¡ç”Ÿç‰¹å¾µï¼‰
    df = pd.DataFrame([data])
    
    # è¡ç”Ÿç‰¹å¾µ
    if 'koi_period' in df.columns and 'koi_duration' in df.columns:
        df['duration_period_ratio'] = df['koi_duration'] / (df['koi_period'] * 24)
    
    if 'koi_depth' in df.columns and 'koi_prad' in df.columns and df['koi_prad'].notna().any():
        df['depth_radius_ratio'] = df['koi_depth'] / (df['koi_prad'] ** 2 + 1e-6)
    
    if 'koi_model_snr' in df.columns and df['koi_model_snr'].notna().any():
        df['log_snr'] = np.log1p(df['koi_model_snr'])
    
    # å¡«è£œç¼ºå¤±å€¼
    df = df.fillna(df.median())
    
    # åªä¿ç•™æ¨¡å‹éœ€è¦çš„ç‰¹å¾µ
    available_features = [f for f in FEATURES if f in df.columns]
    missing_features = set(FEATURES) - set(available_features)
    
    if missing_features:
        print(f"âš ï¸  ç¼ºå°‘ç‰¹å¾µ: {missing_features}")
        # ç”¨ 0 å¡«è£œ
        for feat in missing_features:
            df[feat] = 0
    
    return df[FEATURES]


@app.get("/")
async def root():
    """API æ ¹è·¯å¾‘"""
    return {
        "name": "Exoplanet Detection API",
        "version": "1.0.0",
        "status": "online" if MODEL is not None else "model not loaded",
        "endpoints": {
            "predict": "/predict",
            "predict_batch": "/predict-batch",
            "explain": "/explain",
            "metrics": "/metrics",
            "health": "/health"
        }
    }


@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥"""
    return {
        "status": "healthy" if MODEL is not None else "unhealthy",
        "model_loaded": MODEL is not None,
        "features_count": len(FEATURES) if FEATURES else 0,
        "timestamp": datetime.now().isoformat()
    }


@app.post("/predict", response_model=PredictionResponse)
async def predict(input_data: FeatureInput):
    """å–®ç­†é æ¸¬"""
    if MODEL is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹å°šæœªè¼‰å…¥")
    
    try:
        # æº–å‚™ç‰¹å¾µ
        X = prepare_features(input_data.dict())
        
        # æ¨™æº–åŒ–
        X_scaled = SCALER.transform(X)
        
        # é æ¸¬
        proba = MODEL.predict_proba(X_scaled)[0]
        pred_idx = np.argmax(proba)
        pred_label = LABEL_ENCODER.inverse_transform([pred_idx])[0]
        
        # å»ºç«‹æ©Ÿç‡å­—å…¸
        prob_dict = {
            label: float(prob)
            for label, prob in zip(LABEL_ENCODER.classes_, proba)
        }
        
        return PredictionResponse(
            label=pred_label,
            confidence=float(proba[pred_idx]),
            probabilities=prob_dict,
            timestamp=datetime.now().isoformat()
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é æ¸¬å¤±æ•—: {str(e)}")


@app.post("/predict-batch", response_model=BatchPredictionResponse)
async def predict_batch(file: UploadFile = File(...)):
    """æ‰¹æ¬¡é æ¸¬"""
    if MODEL is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹å°šæœªè¼‰å…¥")
    
    if not file.filename.endswith('.csv'):
        raise HTTPException(status_code=400, detail="åƒ…æ”¯æ´ CSV æ ¼å¼")
    
    try:
        # è®€å– CSV
        contents = await file.read()
        df = pd.read_csv(io.StringIO(contents.decode('utf-8')))
        
        print(f"ğŸ“Š æ‰¹æ¬¡é æ¸¬: {len(df)} ç­†è³‡æ–™")
        
        # æº–å‚™ç‰¹å¾µ
        results = []
        for idx, row in df.iterrows():
            try:
                X = prepare_features(row.to_dict())
                X_scaled = SCALER.transform(X)
                
                proba = MODEL.predict_proba(X_scaled)[0]
                pred_idx = np.argmax(proba)
                pred_label = LABEL_ENCODER.inverse_transform([pred_idx])[0]
                
                results.append({
                    'index': int(idx),
                    'label': pred_label,
                    'confidence': float(proba[pred_idx]),
                    'probabilities': {
                        label: float(prob)
                        for label, prob in zip(LABEL_ENCODER.classes_, proba)
                    }
                })
            except Exception as e:
                results.append({
                    'index': int(idx),
                    'error': str(e)
                })
        
        # çµ±è¨ˆ
        labels = [r['label'] for r in results if 'label' in r]
        summary = {label: labels.count(label) for label in set(labels)}
        
        return BatchPredictionResponse(
            total=len(results),
            predictions=results,
            summary=summary,
            timestamp=datetime.now().isoformat()
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ‰¹æ¬¡é æ¸¬å¤±æ•—: {str(e)}")


@app.post("/explain", response_model=ExplainResponse)
async def explain(input_data: FeatureInput):
    """è§£é‡‹å–®ç­†é æ¸¬"""
    if MODEL is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹å°šæœªè¼‰å…¥")
    
    try:
        # æº–å‚™ç‰¹å¾µ
        X = prepare_features(input_data.dict())
        X_scaled = SCALER.transform(X)
        
        # é æ¸¬
        proba = MODEL.predict_proba(X_scaled)[0]
        pred_idx = np.argmax(proba)
        pred_label = LABEL_ENCODER.inverse_transform([pred_idx])[0]
        
        # ç‰¹å¾µé‡è¦åº¦ï¼ˆç°¡åŒ–ç‰ˆï¼Œä½¿ç”¨æ¨¡å‹ä¿‚æ•¸æˆ–ç‰¹å¾µå€¼ï¼‰
        feature_importance = []
        for feat, val in zip(FEATURES, X_scaled[0]):
            feature_importance.append({
                'feature': feat,
                'value': float(val),
                'abs_value': float(abs(val))
            })
        
        # æ’åº
        feature_importance.sort(key=lambda x: x['abs_value'], reverse=True)
        top_features = feature_importance[:10]
        
        # SHAP valuesï¼ˆå¦‚æœå¯ç”¨ï¼‰
        shap_values = []
        if EXPLAINER is not None:
            try:
                shap_vals = EXPLAINER(X_scaled)
                shap_values = shap_vals.values[0].tolist()
            except:
                pass
        
        return ExplainResponse(
            prediction=pred_label,
            top_features=top_features,
            shap_values=shap_values
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è§£é‡‹å¤±æ•—: {str(e)}")


@app.get("/metrics")
async def get_metrics():
    """ç²å–æ¨¡å‹æŒ‡æ¨™"""
    if MODEL is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹å°šæœªè¼‰å…¥")
    
    return {
        "model_info": {
            "type": "Stacking Ensemble",
            "features_count": len(FEATURES),
            "classes": LABEL_ENCODER.classes_.tolist()
        },
        "features": FEATURES,
        "timestamp": datetime.now().isoformat()
    }


if __name__ == "__main__":
    import uvicorn
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘     ç³»å¤–è¡Œæ˜Ÿæª¢æ¸¬ API æœå‹™                              â•‘
    â•‘     Exoplanet Detection API Server                    â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    uvicorn.run(app, host="0.0.0.0", port=8000)