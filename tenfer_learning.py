"""
è·¨è³‡æ–™é›†é·ç§»å­¸ç¿’
åœ¨ä¸€å€‹è³‡æ–™é›†ä¸Šè¨“ç·´ï¼Œåœ¨å¦ä¸€å€‹è³‡æ–™é›†ä¸Šæ¸¬è©¦æˆ–å¾®èª¿
"""

import numpy as np
import pandas as pd
import joblib
from pathlib import Path
from datetime import datetime

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

from unified_data_loader import UnifiedDataLoader
from train_custom_model import FlexibleExoplanetTrainer, ModelFactory

import warnings
warnings.filterwarnings('ignore')


class TransferLearningPipeline:
    """é·ç§»å­¸ç¿’æµç¨‹"""
    
    def __init__(self):
        self.loader = UnifiedDataLoader()
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.source_model = None
        self.target_model = None
        self.feature_cols = None
    
    def load_source_data(self, filepath, dataset_type=None, label_strategy='binary'):
        """è¼‰å…¥æºè³‡æ–™é›†ï¼ˆç”¨æ–¼è¨“ç·´ï¼‰"""
        print("\n" + "="*70)
        print("ğŸ“š è¼‰å…¥æºè³‡æ–™é›† (Source Dataset)")
        print("="*70)
        
        df, detected_type = self.loader.load_dataset(filepath, dataset_type, label_strategy)
        X, y, groups = self.loader.get_features_and_labels(df, detected_type)
        
        self.source_dataset_type = detected_type
        self.feature_cols = X.columns.tolist()
        
        print(f"\nâœ“ æºè³‡æ–™é›†: {detected_type}")
        print(f"âœ“ è³‡æ–™å½¢ç‹€: {X.shape}")
        print(f"âœ“ é¡åˆ¥åˆ†ä½ˆ:\n{y.value_counts()}")
        
        return X, y, groups
    
    def load_target_data(self, filepath, dataset_type=None, label_strategy='binary'):
        """è¼‰å…¥ç›®æ¨™è³‡æ–™é›†ï¼ˆç”¨æ–¼æ¸¬è©¦æˆ–å¾®èª¿ï¼‰"""
        print("\n" + "="*70)
        print("ğŸ¯ è¼‰å…¥ç›®æ¨™è³‡æ–™é›† (Target Dataset)")
        print("="*70)
        
        df, detected_type = self.loader.load_dataset(filepath, dataset_type, label_strategy)
        X, y, groups = self.loader.get_features_and_labels(df, detected_type)
        
        # ç¢ºä¿ç›®æ¨™è³‡æ–™é›†ä½¿ç”¨ç›¸åŒçš„ç‰¹å¾µ
        missing_features = set(self.feature_cols) - set(X.columns)
        if missing_features:
            print(f"\nâš ï¸  ç›®æ¨™è³‡æ–™é›†ç¼ºå°‘ä»¥ä¸‹ç‰¹å¾µ: {missing_features}")
            print("   å°‡ç”¨ 0 å¡«å……ç¼ºå¤±ç‰¹å¾µ")
            for feat in missing_features:
                X[feat] = 0
        
        # ç¢ºä¿ç‰¹å¾µé †åºä¸€è‡´
        X = X[self.feature_cols]
        
        self.target_dataset_type = detected_type
        
        print(f"\nâœ“ ç›®æ¨™è³‡æ–™é›†: {detected_type}")
        print(f"âœ“ è³‡æ–™å½¢ç‹€: {X.shape}")
        if y is not None:
            print(f"âœ“ é¡åˆ¥åˆ†ä½ˆ:\n{y.value_counts()}")
        
        return X, y, groups
    
    def train_source_model(self, X_source, y_source, model_type='lgbm', **model_params):
        """åœ¨æºè³‡æ–™é›†ä¸Šè¨“ç·´æ¨¡å‹"""
        print("\n" + "="*70)
        print("ğŸš€ åœ¨æºè³‡æ–™é›†ä¸Šè¨“ç·´æ¨¡å‹")
        print("="*70)
        
        # æ¨™æº–åŒ–
        X_scaled = self.scaler.fit_transform(X_source)
        X_scaled = pd.DataFrame(X_scaled, columns=X_source.columns)
        
        # ç·¨ç¢¼æ¨™ç±¤
        y_encoded = self.label_encoder.fit_transform(y_source)
        
        print(f"\né¡åˆ¥å°æ‡‰:")
        for i, label in enumerate(self.label_encoder.classes_):
            print(f"  {i}: {label}")
        
        # å»ºç«‹ä¸¦è¨“ç·´æ¨¡å‹
        print(f"\nğŸ—ï¸  å»ºç«‹æ¨¡å‹: {model_type}")
        self.source_model = ModelFactory.get_model(model_type, **model_params)
        
        print("\nğŸ“Š è¨“ç·´ä¸­...")
        self.source_model.fit(X_scaled, y_encoded)
        
        # è©•ä¼°æºè³‡æ–™é›†æ€§èƒ½
        train_preds = self.source_model.predict(X_scaled)
        train_acc = accuracy_score(y_encoded, train_preds)
        
        print(f"\nâœ“ æºè³‡æ–™é›†è¨“ç·´å®Œæˆ")
        print(f"âœ“ è¨“ç·´æº–ç¢ºç‡: {train_acc:.4f}")
        
        print("\nè¨“ç·´é›†åˆ†é¡å ±å‘Š:")
        print(classification_report(
            y_encoded, train_preds,
            target_names=self.label_encoder.classes_
        ))
        
        return self.source_model
    
    def test_on_target(self, X_target, y_target):
        """ç›´æ¥åœ¨ç›®æ¨™è³‡æ–™é›†ä¸Šæ¸¬è©¦ï¼ˆé›¶æ¨£æœ¬é·ç§»ï¼‰"""
        print("\n" + "="*70)
        print("ğŸ”¬ é›¶æ¨£æœ¬é·ç§»æ¸¬è©¦ (Zero-shot Transfer)")
        print("="*70)
        print("èªªæ˜: ç›´æ¥ç”¨æºæ¨¡å‹é æ¸¬ç›®æ¨™è³‡æ–™ï¼Œä¸åšä»»ä½•å¾®èª¿")
        
        # æ¨™æº–åŒ–ç›®æ¨™è³‡æ–™
        X_scaled = self.scaler.transform(X_target)
        X_scaled = pd.DataFrame(X_scaled, columns=X_target.columns)
        
        # ç·¨ç¢¼ç›®æ¨™æ¨™ç±¤
        y_encoded = self.label_encoder.transform(y_target)
        
        # é æ¸¬
        preds = self.source_model.predict(X_scaled)
        probs = self.source_model.predict_proba(X_scaled)
        
        # è©•ä¼°
        acc = accuracy_score(y_encoded, preds)
        
        print(f"\nç›®æ¨™è³‡æ–™é›†æ€§èƒ½:")
        print(f"æº–ç¢ºç‡: {acc:.4f}")
        
        print("\nåˆ†é¡å ±å‘Š:")
        print(classification_report(
            y_encoded, preds,
            target_names=self.label_encoder.classes_
        ))
        
        print("\næ··æ·†çŸ©é™£:")
        cm = confusion_matrix(y_encoded, preds)
        print(cm)
        
        return acc, preds, probs
    
    def fine_tune_on_target(self, X_target, y_target, model_type='lgbm', fine_tune_ratio=0.2, **model_params):
        """åœ¨ç›®æ¨™è³‡æ–™é›†ä¸Šå¾®èª¿ï¼ˆé·ç§»å­¸ç¿’ï¼‰"""
        print("\n" + "="*70)
        print("ğŸ¨ é·ç§»å­¸ç¿’å¾®èª¿ (Fine-tuning)")
        print("="*70)
        print(f"èªªæ˜: ç”¨ç›®æ¨™è³‡æ–™é›†çš„ {fine_tune_ratio*100:.0f}% é€²è¡Œå¾®èª¿ï¼Œå‰©é¤˜éƒ¨åˆ†æ¸¬è©¦")
        
        # åˆ†å‰²ç›®æ¨™è³‡æ–™é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X_target, y_target, 
            test_size=1-fine_tune_ratio,
            random_state=42,
            stratify=y_target
        )
        
        print(f"\nè³‡æ–™åˆ†å‰²:")
        print(f"  å¾®èª¿é›†: {len(X_train)} ç­†")
        print(f"  æ¸¬è©¦é›†: {len(X_test)} ç­†")
        
        # æ¨™æº–åŒ–
        X_train_scaled = self.scaler.transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)
        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)
        
        # ç·¨ç¢¼æ¨™ç±¤
        y_train_encoded = self.label_encoder.transform(y_train)
        y_test_encoded = self.label_encoder.transform(y_test)
        
        # å¾®èª¿æ¨¡å‹
        print(f"\nğŸ”§ å¾®èª¿æ¨¡å‹...")
        self.target_model = ModelFactory.get_model(model_type, **model_params)
        
        # é¸é …1: å¾é ­è¨“ç·´ï¼ˆä½¿ç”¨è¼ƒå°‘è³‡æ–™ï¼‰
        self.target_model.fit(X_train_scaled, y_train_encoded)
        
        # è©•ä¼°
        train_preds = self.target_model.predict(X_train_scaled)
        test_preds = self.target_model.predict(X_test_scaled)
        
        train_acc = accuracy_score(y_train_encoded, train_preds)
        test_acc = accuracy_score(y_test_encoded, test_preds)
        
        print(f"\nâœ“ å¾®èª¿å®Œæˆ")
        print(f"âœ“ å¾®èª¿é›†æº–ç¢ºç‡: {train_acc:.4f}")
        print(f"âœ“ æ¸¬è©¦é›†æº–ç¢ºç‡: {test_acc:.4f}")
        
        print("\næ¸¬è©¦é›†åˆ†é¡å ±å‘Š:")
        print(classification_report(
            y_test_encoded, test_preds,
            target_names=self.label_encoder.classes_
        ))
        
        print("\næ··æ·†çŸ©é™£:")
        cm = confusion_matrix(y_test_encoded, test_preds)
        print(cm)
        
        return test_acc, self.target_model
    
    def compare_strategies(self, X_target, y_target, model_type='lgbm', **model_params):
        """æ¯”è¼ƒä¸åŒé·ç§»ç­–ç•¥çš„æ•ˆæœ"""
        print("\n" + "="*70)
        print("ğŸ“Š é·ç§»å­¸ç¿’ç­–ç•¥æ¯”è¼ƒ")
        print("="*70)
        
        results = {}
        
        # ç­–ç•¥1: é›¶æ¨£æœ¬é·ç§»
        print("\nã€ç­–ç•¥1ã€‘é›¶æ¨£æœ¬é·ç§»ï¼ˆä¸ä½¿ç”¨ç›®æ¨™è³‡æ–™è¨“ç·´ï¼‰")
        zero_shot_acc, _, _ = self.test_on_target(X_target, y_target)
        results['zero_shot'] = zero_shot_acc
        
        # ç­–ç•¥2: ç”¨10%ç›®æ¨™è³‡æ–™å¾®èª¿
        print("\nã€ç­–ç•¥2ã€‘ç”¨ 10% ç›®æ¨™è³‡æ–™å¾®èª¿")
        fine_tune_10_acc, _ = self.fine_tune_on_target(
            X_target, y_target, model_type, fine_tune_ratio=0.1, **model_params
        )
        results['fine_tune_10'] = fine_tune_10_acc
        
        # ç­–ç•¥3: ç”¨30%ç›®æ¨™è³‡æ–™å¾®èª¿
        print("\nã€ç­–ç•¥3ã€‘ç”¨ 30% ç›®æ¨™è³‡æ–™å¾®èª¿")
        fine_tune_30_acc, _ = self.fine_tune_on_target(
            X_target, y_target, model_type, fine_tune_ratio=0.3, **model_params
        )
        results['fine_tune_30'] = fine_tune_30_acc
        
        # ç­–ç•¥4: å®Œå…¨åœ¨ç›®æ¨™è³‡æ–™ä¸Šè¨“ç·´ï¼ˆåŸºæº–ï¼‰
        print("\nã€ç­–ç•¥4ã€‘å®Œå…¨åœ¨ç›®æ¨™è³‡æ–™ä¸Šè¨“ç·´ï¼ˆåŸºæº–ï¼‰")
        X_train, X_test, y_train, y_test = train_test_split(
            X_target, y_target, test_size=0.3, random_state=42, stratify=y_target
        )
        
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        y_train_encoded = self.label_encoder.fit_transform(y_train)
        y_test_encoded = self.label_encoder.transform(y_test)
        
        target_only_model = ModelFactory.get_model(model_type, **model_params)
        target_only_model.fit(X_train_scaled, y_train_encoded)
        
        test_preds = target_only_model.predict(X_test_scaled)
        target_only_acc = accuracy_score(y_test_encoded, test_preds)
        results['target_only'] = target_only_acc
        
        print(f"\nâœ“ æ¸¬è©¦é›†æº–ç¢ºç‡: {target_only_acc:.4f}")
        
        # ç¸½çµæ¯”è¼ƒ
        print("\n" + "="*70)
        print("ğŸ“ˆ é·ç§»å­¸ç¿’æ•ˆæœç¸½çµ")
        print("="*70)
        
        print(f"\nç­–ç•¥                           æº–ç¢ºç‡      æå‡")
        print("-" * 60)
        baseline = results['target_only']
        for strategy, acc in results.items():
            improvement = acc - baseline
            improvement_pct = (improvement / baseline) * 100 if baseline > 0 else 0
            
            strategy_names = {
                'zero_shot': 'é›¶æ¨£æœ¬é·ç§» (0% ç›®æ¨™è³‡æ–™)',
                'fine_tune_10': 'å¾®èª¿ (10% ç›®æ¨™è³‡æ–™)',
                'fine_tune_30': 'å¾®èª¿ (30% ç›®æ¨™è³‡æ–™)',
                'target_only': 'å®Œå…¨ç›®æ¨™è¨“ç·´ (70% è³‡æ–™) ã€åŸºæº–ã€‘'
            }
            
            name = strategy_names.get(strategy, strategy)
            print(f"{name:30s} {acc:6.4f}    {improvement:+.4f} ({improvement_pct:+.1f}%)")
        
        print("\nçµè«–:")
        if results['zero_shot'] > baseline * 0.8:
            print("âœ“ æºæ¨¡å‹åœ¨ç›®æ¨™è³‡æ–™é›†ä¸Šè¡¨ç¾ä¸éŒ¯ï¼Œé·ç§»å­¸ç¿’æœ‰æ•ˆï¼")
        else:
            print("âš ï¸  æºæ¨¡å‹ç›´æ¥é·ç§»æ•ˆæœæœ‰é™ï¼Œéœ€è¦å¾®èª¿")
        
        if results['fine_tune_10'] > results['zero_shot']:
            print("âœ“ å°‘é‡ç›®æ¨™è³‡æ–™å¾®èª¿å¯ä»¥æå‡æ€§èƒ½")
        
        if results['fine_tune_30'] > results['target_only']:
            print("âœ“ é·ç§»å­¸ç¿’å„ªæ–¼å¾é ­è¨“ç·´ï¼Œè­‰æ˜æºè³‡æ–™é›†çš„çŸ¥è­˜æœ‰å¹«åŠ©ï¼")
        
        return results
    
    def save_transfer_model(self, model_name='transfer_model'):
        """å„²å­˜é·ç§»å­¸ç¿’æ¨¡å‹"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        save_dict = {
            'source_model': self.source_model,
            'target_model': self.target_model,
            'scaler': self.scaler,
            'label_encoder': self.label_encoder,
            'feature_cols': self.feature_cols,
            'source_dataset': self.source_dataset_type,
            'target_dataset': self.target_dataset_type,
            'timestamp': timestamp
        }
        
        model_path = Path("models") / f"{model_name}_{timestamp}.pkl"
        joblib.dump(save_dict, model_path)
        
        print(f"\nğŸ’¾ é·ç§»æ¨¡å‹å·²å„²å­˜: {model_path}")
        return model_path


def main():
    """ä¸»ç¨‹å¼ - äº’å‹•å¼é·ç§»å­¸ç¿’"""
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                       â•‘
    â•‘     Transfer Learning for Exoplanet Detection         â•‘
    â•‘                                                       â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    pipeline = TransferLearningPipeline()
    
    print("\né·ç§»å­¸ç¿’å¯¦é©—è¨­å®š:")
    print("="*70)
    
    # é¸æ“‡æºè³‡æ–™é›†
    print("\næºè³‡æ–™é›†ï¼ˆç”¨æ–¼è¨“ç·´ï¼‰:")
    print("1. TESS (./tois.csv)")
    print("2. Kepler (./data/kepler_koi.csv)")
    print("3. è‡ªè¨‚è·¯å¾‘")
    
    source_choice = input("\nè«‹é¸æ“‡æºè³‡æ–™é›† (1-3): ").strip()
    
    if source_choice == '1':
        source_file = './tois.csv'
    elif source_choice == '2':
        source_file = './data/kepler_koi.csv'
    else:
        source_file = input("è«‹è¼¸å…¥æºè³‡æ–™é›†è·¯å¾‘: ").strip()
    
    # é¸æ“‡ç›®æ¨™è³‡æ–™é›†
    print("\nç›®æ¨™è³‡æ–™é›†ï¼ˆç”¨æ–¼æ¸¬è©¦/å¾®èª¿ï¼‰:")
    print("1. TESS (./tois.csv)")
    print("2. Kepler (./data/kepler_koi.csv)")
    print("3. è‡ªè¨‚è·¯å¾‘")
    
    target_choice = input("\nè«‹é¸æ“‡ç›®æ¨™è³‡æ–™é›† (1-3): ").strip()
    
    if target_choice == '1':
        target_file = './tois.csv'
    elif target_choice == '2':
        target_file = './data/kepler_koi.csv'
    else:
        target_file = input("è«‹è¼¸å…¥ç›®æ¨™è³‡æ–™é›†è·¯å¾‘: ").strip()
    
    # é¸æ“‡æ¨™ç±¤ç­–ç•¥
    print("\næ¨™ç±¤ç­–ç•¥:")
    print("1. äºŒåˆ†é¡ (PLANET vs NOT_PLANET)")
    print("2. ä¸‰åˆ†é¡ (PLANET, FALSE_POSITIVE, OTHER)")
    
    label_choice = input("\nè«‹é¸æ“‡ (1-2, é è¨­=1): ").strip()
    label_strategy = 'binary' if label_choice != '2' else 'three_class'
    
    # é¸æ“‡æ¨¡å‹
    print("\næ¨¡å‹é¡å‹:")
    print("1. LightGBM (å¿«é€Ÿ)")
    print("2. XGBoost (æº–ç¢º)")
    print("3. Random Forest (ç©©å®š)")
    
    model_choice = input("\nè«‹é¸æ“‡ (1-3, é è¨­=1): ").strip()
    model_map = {'1': 'lgbm', '2': 'xgb', '3': 'random_forest'}
    model_type = model_map.get(model_choice, 'lgbm')
    
    # è¼‰å…¥è³‡æ–™
    X_source, y_source, groups_source = pipeline.load_source_data(
        source_file, label_strategy=label_strategy
    )
    
    X_target, y_target, groups_target = pipeline.load_target_data(
        target_file, label_strategy=label_strategy
    )
    
    # è¨“ç·´æºæ¨¡å‹
    pipeline.train_source_model(X_source, y_source, model_type, n_estimators=200)
    
    # é¸æ“‡é·ç§»ç­–ç•¥
    print("\né·ç§»å­¸ç¿’æ¨¡å¼:")
    print("1. é›¶æ¨£æœ¬æ¸¬è©¦ï¼ˆç›´æ¥æ¸¬è©¦ï¼Œä¸å¾®èª¿ï¼‰")
    print("2. å¾®èª¿æ¨¡å‹ï¼ˆç”¨éƒ¨åˆ†ç›®æ¨™è³‡æ–™å¾®èª¿ï¼‰")
    print("3. å®Œæ•´æ¯”è¼ƒï¼ˆæ¸¬è©¦æ‰€æœ‰ç­–ç•¥ï¼‰")
    
    transfer_choice = input("\nè«‹é¸æ“‡ (1-3, é è¨­=3): ").strip()
    
    if transfer_choice == '1':
        pipeline.test_on_target(X_target, y_target)
    
    elif transfer_choice == '2':
        ratio = input("\nå¾®èª¿è³‡æ–™æ¯”ä¾‹ (0.1-0.5, é è¨­=0.2): ").strip()
        ratio = float(ratio) if ratio else 0.2
        pipeline.fine_tune_on_target(X_target, y_target, model_type, ratio)
    
    else:
        pipeline.compare_strategies(X_target, y_target, model_type)
    
    # å„²å­˜
    save_choice = input("\næ˜¯å¦å„²å­˜é·ç§»æ¨¡å‹? (y/n): ").strip()
    if save_choice.lower() == 'y':
        pipeline.save_transfer_model()
    
    print("\n" + "="*70)
    print("âœ… é·ç§»å­¸ç¿’å¯¦é©—å®Œæˆï¼")
    print("="*70)


if __name__ == "__main__":
    main()