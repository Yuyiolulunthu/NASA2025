"""
éˆæ´»çš„ç³»å¤–è¡Œæ˜Ÿæ¨¡å‹è¨“ç·´å™¨
æ”¯æ´å¤šç¨®æ¨¡å‹é…ç½®å’Œè¨“ç·´ç­–ç•¥
"""

import numpy as np
import pandas as pd
import joblib
import json
from pathlib import Path
from datetime import datetime

from sklearn.model_selection import GroupKFold, cross_val_predict, train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.calibration import CalibratedClassifierCV

from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.ensemble import RandomForestClassifier, StackingClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier

import warnings
warnings.filterwarnings('ignore')


class ModelFactory:
    """æ¨¡å‹å·¥å»  - å¯ä»¥å»ºç«‹å„ç¨®ä¸åŒçš„æ¨¡å‹"""
    
    @staticmethod
    def get_model(model_type, **kwargs):
        """
        ç²å–æŒ‡å®šé¡å‹çš„æ¨¡å‹
        
        å¯ç”¨æ¨¡å‹:
        - 'lgbm': LightGBM
        - 'xgb': XGBoost
        - 'catboost': CatBoost
        - 'random_forest': éš¨æ©Ÿæ£®æ—
        - 'gradient_boosting': æ¢¯åº¦æå‡
        - 'logistic': é‚è¼¯è¿´æ­¸
        - 'svm': æ”¯æŒå‘é‡æ©Ÿ
        - 'mlp': ç¥ç¶“ç¶²è·¯
        - 'stacking': å †ç–Šæ¨¡å‹ï¼ˆè‡ªå‹•çµ„åˆå¤šå€‹æ¨¡å‹ï¼‰
        """
        
        models = {
            'lgbm': lambda: LGBMClassifier(
                n_estimators=kwargs.get('n_estimators', 200),
                learning_rate=kwargs.get('learning_rate', 0.05),
                max_depth=kwargs.get('max_depth', 5),
                random_state=42,
                verbose=-1,
                n_jobs=1
            ),
            
            'xgb': lambda: XGBClassifier(
                n_estimators=kwargs.get('n_estimators', 200),
                learning_rate=kwargs.get('learning_rate', 0.05),
                max_depth=kwargs.get('max_depth', 5),
                random_state=42,
                eval_metric='logloss',
                verbosity=0,
                n_jobs=1
            ),
            
            'catboost': lambda: CatBoostClassifier(
                iterations=kwargs.get('n_estimators', 200),
                learning_rate=kwargs.get('learning_rate', 0.05),
                depth=kwargs.get('max_depth', 5),
                random_state=42,
                verbose=False,
                thread_count=1
            ),
            
            'random_forest': lambda: RandomForestClassifier(
                n_estimators=kwargs.get('n_estimators', 200),
                max_depth=kwargs.get('max_depth', 10),
                class_weight='balanced',
                random_state=42,
                n_jobs=1
            ),
            
            'gradient_boosting': lambda: GradientBoostingClassifier(
                n_estimators=kwargs.get('n_estimators', 200),
                learning_rate=kwargs.get('learning_rate', 0.05),
                max_depth=kwargs.get('max_depth', 5),
                random_state=42
            ),
            
            'logistic': lambda: LogisticRegression(
                max_iter=kwargs.get('max_iter', 1000),
                class_weight='balanced',
                random_state=42
            ),
            
            'svm': lambda: SVC(
                kernel=kwargs.get('kernel', 'rbf'),
                C=kwargs.get('C', 1.0),
                class_weight='balanced',
                probability=True,
                random_state=42
            ),
            
            'mlp': lambda: MLPClassifier(
                hidden_layer_sizes=kwargs.get('hidden_layers', (100, 50)),
                max_iter=kwargs.get('max_iter', 500),
                random_state=42
            ),
            
            'stacking': lambda: ModelFactory._build_stacking_model(**kwargs)
        }
        
        if model_type not in models:
            raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹é¡å‹: {model_type}. å¯ç”¨: {list(models.keys())}")
        
        return models[model_type]()
    
    @staticmethod
    def _build_stacking_model(**kwargs):
        """å»ºç«‹å †ç–Šæ¨¡å‹"""
        base_learners = [
            ('lgbm', LGBMClassifier(
                n_estimators=200,
                learning_rate=0.05,
                max_depth=5,
                random_state=42,
                verbose=-1,
                n_jobs=1
            )),
            ('xgb', XGBClassifier(
                n_estimators=200,
                learning_rate=0.05,
                max_depth=5,
                random_state=42,
                eval_metric='logloss',
                verbosity=0,
                n_jobs=1
            )),
            ('catboost', CatBoostClassifier(
                iterations=200,
                learning_rate=0.05,
                depth=5,
                random_state=42,
                verbose=False,
                thread_count=1
            )),
            ('rf', RandomForestClassifier(
                n_estimators=200,
                max_depth=10,
                class_weight='balanced',
                random_state=42,
                n_jobs=1
            ))
        ]
        
        meta_learner = LogisticRegression(
            max_iter=1000,
            class_weight='balanced',
            random_state=42
        )
        
        return StackingClassifier(
            estimators=base_learners,
            final_estimator=meta_learner,
            cv=3,
            n_jobs=1
        )


class FlexibleExoplanetTrainer:
    """éˆæ´»çš„ç³»å¤–è¡Œæ˜Ÿè¨“ç·´å™¨"""
    
    def __init__(self, model_type='stacking', **model_params):
        """
        åˆå§‹åŒ–è¨“ç·´å™¨
        
        Args:
            model_type: æ¨¡å‹é¡å‹ ('lgbm', 'xgb', 'random_forest', 'stacking', ç­‰)
            **model_params: æ¨¡å‹çš„è¶…åƒæ•¸
        """
        self.model_type = model_type
        self.model_params = model_params
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.selected_features = None
        self.model = None
        
        Path("models").mkdir(exist_ok=True)
        
        print(f"ğŸ¯ é¸æ“‡æ¨¡å‹: {model_type}")
        if model_params:
            print(f"ğŸ“ æ¨¡å‹åƒæ•¸: {model_params}")
    
    def load_data(self, filepath='./tois.csv', label_strategy='binary'):
        """
        è¼‰å…¥è³‡æ–™
        
        Args:
            filepath: è³‡æ–™æª”æ¡ˆè·¯å¾‘
            label_strategy: æ¨™ç±¤ç­–ç•¥
                - 'binary': äºŒåˆ†é¡ (PLANET vs NOT_PLANET)
                - 'three_class': ä¸‰åˆ†é¡ (PLANET, FALSE_POSITIVE, OTHER)
                - 'full': ä¿ç•™æ‰€æœ‰åŸå§‹é¡åˆ¥
        """
        print(f"\nğŸ“¥ è¼‰å…¥è³‡æ–™: {filepath}")
        print(f"ğŸ·ï¸  æ¨™ç±¤ç­–ç•¥: {label_strategy}")
        
        df = pd.read_csv(filepath)
        print(f"âœ“ åŸå§‹è³‡æ–™: {df.shape}")
        
        # æ‰¾åˆ°æ¨™ç±¤æ¬„ä½
        if 'TESS Disposition' in df.columns:
            disp_col = 'TESS Disposition'
        elif 'TFOPWG Disposition' in df.columns:
            disp_col = 'TFOPWG Disposition'
        else:
            raise ValueError("æ‰¾ä¸åˆ°æ¨™ç±¤æ¬„ä½")
        
        # æ ¹æ“šç­–ç•¥è¨­å®šæ¨™ç±¤
        df = df.dropna(subset=[disp_col])
        
        if label_strategy == 'binary':
            label_mapping = {
                'KP': 'PLANET', 'CP': 'PLANET', 'PC': 'PLANET',
                'EB': 'NOT_PLANET', 'FP': 'NOT_PLANET',
                'IS': 'NOT_PLANET', 'V': 'NOT_PLANET', 'O': 'NOT_PLANET'
            }
        elif label_strategy == 'three_class':
            label_mapping = {
                'KP': 'PLANET', 'CP': 'PLANET', 'PC': 'PLANET',
                'EB': 'FALSE_POSITIVE', 'FP': 'FALSE_POSITIVE',
                'IS': 'OTHER', 'V': 'OTHER', 'O': 'OTHER'
            }
        else:  # full
            label_mapping = None
            df['disposition'] = df[disp_col]
        
        if label_mapping:
            df['disposition'] = df[disp_col].map(label_mapping)
            df = df.dropna(subset=['disposition'])
        
        print(f"âœ“ éæ¿¾å¾Œè³‡æ–™: {df.shape}")
        print(f"\né¡åˆ¥åˆ†ä½ˆ:")
        print(df['disposition'].value_counts())
        
        return df
    
    def engineer_features(self, df):
        """ç‰¹å¾µå·¥ç¨‹"""
        print("\nğŸ”§ ç‰¹å¾µå·¥ç¨‹...")
        
        feature_mapping = {
            'Period (days)': 'period',
            'Duration (hours)': 'duration',
            'Depth (ppm)': 'depth',
            'Planet Radius (R_Earth)': 'planet_radius',
            'Planet Equil Temp (K)': 'equil_temp',
            'Planet Insolation (Earth Flux)': 'insolation',
            'Planet SNR': 'snr',
            'Stellar Eff Temp (K)': 'stellar_temp',
            'Stellar Radius (R_Sun)': 'stellar_radius',
            'Stellar log(g) (cm/s^2)': 'stellar_logg',
            'Stellar Mass (M_Sun)': 'stellar_mass',
            'TESS Mag': 'tess_mag',
            'Stellar Distance (pc)': 'distance'
        }
        
        available_features = {k: v for k, v in feature_mapping.items() if k in df.columns}
        X = df[list(available_features.keys())].copy()
        X.columns = list(available_features.values())
        
        # è™•ç†ç¼ºå¤±å€¼
        for col in X.columns:
            if X[col].isnull().any():
                X[col].fillna(X[col].median(), inplace=True)
        
        # è¡ç”Ÿç‰¹å¾µ
        if 'period' in X.columns and 'duration' in X.columns:
            X['duration_period_ratio'] = X['duration'] / (X['period'] * 24 + 1e-6)
        
        if 'depth' in X.columns and 'planet_radius' in X.columns:
            X['depth_radius_ratio'] = X['depth'] / (X['planet_radius'] ** 2 + 1e-6)
        
        if 'snr' in X.columns:
            X['log_snr'] = np.log1p(X['snr'])
        
        if 'equil_temp' in X.columns and 'insolation' in X.columns:
            X['temp_insol_ratio'] = X['equil_temp'] / (X['insolation'] + 1e-6)
        
        if 'stellar_temp' in X.columns and 'stellar_radius' in X.columns:
            X['stellar_luminosity'] = (X['stellar_radius'] ** 2) * ((X['stellar_temp'] / 5778) ** 4)
        
        X = X.replace([np.inf, -np.inf], np.nan)
        X = X.fillna(X.median())
        
        print(f"âœ“ ç‰¹å¾µå·¥ç¨‹å®Œæˆ: {X.shape[1]} å€‹ç‰¹å¾µ")
        
        # å„²å­˜æ‰€æœ‰ç‰¹å¾µï¼ˆä¸åšç‰¹å¾µé¸æ“‡ï¼Œè®“æ¨¡å‹è‡ªå·±å­¸ç¿’ï¼‰
        self.selected_features = X.columns.tolist()
        
        return X
    
    def train(self, X, y, groups=None, use_cross_validation=True):
        """è¨“ç·´æ¨¡å‹"""
        print("\n" + "="*60)
        print("ğŸš€ é–‹å§‹è¨“ç·´")
        print("="*60)
        
        # æ¨™æº–åŒ–
        X_scaled = self.scaler.fit_transform(X)
        X_scaled = pd.DataFrame(X_scaled, columns=X.columns)
        
        # ç·¨ç¢¼æ¨™ç±¤
        y_encoded = self.label_encoder.fit_transform(y)
        print(f"\né¡åˆ¥å°æ‡‰:")
        for i, label in enumerate(self.label_encoder.classes_):
            print(f"  {i}: {label}")
        
        # å»ºç«‹æ¨¡å‹
        print(f"\nğŸ—ï¸  å»ºç«‹æ¨¡å‹: {self.model_type}")
        model = ModelFactory.get_model(self.model_type, **self.model_params)
        
        if use_cross_validation and groups is not None:
            # äº¤å‰é©—è­‰
            print(f"\nğŸ“Š 5-Fold äº¤å‰é©—è­‰...")
            gkf = GroupKFold(n_splits=5)
            
            oof_preds = cross_val_predict(
                model, X_scaled, y_encoded,
                cv=gkf.split(X_scaled, y_encoded, groups),
                method='predict_proba',
                n_jobs=1,
                verbose=1
            )
            
            # è©•ä¼°äº¤å‰é©—è­‰çµæœ
            self._evaluate(y_encoded, oof_preds, "äº¤å‰é©—è­‰")
        
        # å®Œæ•´è¨“ç·´
        print("\nğŸ¯ å®Œæ•´è³‡æ–™è¨“ç·´...")
        model.fit(X_scaled, y_encoded)
        
        # æ©Ÿç‡æ ¡æº– - æª¢æŸ¥æ¯å€‹é¡åˆ¥çš„æ¨£æœ¬æ•¸
        print("\nâš–ï¸  æ©Ÿç‡æ ¡æº–...")
        min_samples = min(np.bincount(y_encoded))
        
        if min_samples >= 3:
            # å‹•æ…‹èª¿æ•´ cv å€¼
            cv_folds = min(3, min_samples)
            print(f"   ä½¿ç”¨ {cv_folds}-fold äº¤å‰é©—è­‰")
            
            calibrated_model = CalibratedClassifierCV(
                model,
                method='isotonic',
                cv=cv_folds
            )
            calibrated_model.fit(X_scaled, y_encoded)
            self.model = calibrated_model
        else:
            print(f"   âš ï¸  æŸäº›é¡åˆ¥æ¨£æœ¬æ•¸å¤ªå°‘ (æœ€å°‘={min_samples})ï¼Œè·³éæ©Ÿç‡æ ¡æº–")
            self.model = model
        
        # åœ¨è¨“ç·´é›†ä¸Šè©•ä¼°
        train_preds = self.model.predict_proba(X_scaled)
        self._evaluate(y_encoded, train_preds, "è¨“ç·´é›†")
        
        print("\nâœ… è¨“ç·´å®Œæˆï¼")
        return self
    
    def _evaluate(self, y_true, y_pred_proba, dataset_name=""):
        """è©•ä¼°æ¨¡å‹"""
        print("\n" + "="*60)
        print(f"ğŸ“ˆ æ¨¡å‹è©•ä¼° - {dataset_name}")
        print("="*60)
        
        y_pred = np.argmax(y_pred_proba, axis=1)
        
        print("\nåˆ†é¡å ±å‘Š:")
        print(classification_report(
            y_true, y_pred,
            target_names=self.label_encoder.classes_
        ))
        
        print("\næ··æ·†çŸ©é™£:")
        cm = confusion_matrix(y_true, y_pred)
        print(cm)
        
        print("\nå„é¡åˆ¥æº–ç¢ºç‡:")
        for i, label in enumerate(self.label_encoder.classes_):
            if cm[i].sum() > 0:
                acc = cm[i, i] / cm[i].sum()
                print(f"  {label}: {acc:.2%}")
        
        try:
            roc_auc = roc_auc_score(
                y_true, y_pred_proba,
                multi_class='ovr',
                average='weighted'
            )
            print(f"\nWeighted ROC-AUC: {roc_auc:.4f}")
        except:
            pass
    
    def save(self, version=None):
        """å„²å­˜æ¨¡å‹"""
        version = version or datetime.now().strftime("%Y%m%d_%H%M%S")
        
        save_dict = {
            'model': self.model,
            'scaler': self.scaler,
            'label_encoder': self.label_encoder,
            'selected_features': self.selected_features,
            'model_type': self.model_type,
            'model_params': self.model_params,
            'version': version,
            'timestamp': datetime.now().isoformat(),
            'data_source': 'NASA TESS TOI'
        }
        
        model_name = f"exoplanet_{self.model_type}_{version}.pkl"
        model_path = Path("models") / model_name
        joblib.dump(save_dict, model_path)
        
        print(f"\nğŸ’¾ æ¨¡å‹å·²å„²å­˜: {model_path}")
        
        return model_path


def main():
    """ä¸»è¨“ç·´æµç¨‹"""
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                       â•‘
    â•‘     Flexible Exoplanet Model Training                 â•‘
    â•‘                                                       â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    print("å¯ç”¨çš„æ¨¡å‹:")
    print("1. LightGBM (lgbm)")
    print("2. XGBoost (xgb)")
    print("3. CatBoost (catboost)")
    print("4. Random Forest (random_forest)")
    print("5. Gradient Boosting (gradient_boosting)")
    print("6. Logistic Regression (logistic)")
    print("7. SVM (svm)")
    print("8. Neural Network (mlp)")
    print("9. Stacking Ensemble (stacking)")
    
    model_choice = input("\nè«‹é¸æ“‡æ¨¡å‹ (1-9, é è¨­=9): ").strip()
    
    model_map = {
        '1': 'lgbm', '2': 'xgb', '3': 'catboost',
        '4': 'random_forest', '5': 'gradient_boosting',
        '6': 'logistic', '7': 'svm', '8': 'mlp', '9': 'stacking'
    }
    
    model_type = model_map.get(model_choice, 'stacking')
    
    print("\næ¨™ç±¤ç­–ç•¥:")
    print("1. äºŒåˆ†é¡ (PLANET vs NOT_PLANET)")
    print("2. ä¸‰åˆ†é¡ (PLANET, FALSE_POSITIVE, OTHER)")
    print("3. å®Œæ•´åˆ†é¡ (ä¿ç•™æ‰€æœ‰åŸå§‹é¡åˆ¥)")
    
    label_choice = input("\nè«‹é¸æ“‡æ¨™ç±¤ç­–ç•¥ (1-3, é è¨­=2): ").strip()
    
    label_map = {
        '1': 'binary',
        '2': 'three_class',
        '3': 'full'
    }
    
    label_strategy = label_map.get(label_choice, 'three_class')
    
    # æ¨¡å‹åƒæ•¸ï¼ˆå¯ä»¥æ ¹æ“šéœ€è¦èª¿æ•´ï¼‰
    model_params = {}
    
    if model_type in ['lgbm', 'xgb', 'catboost', 'random_forest', 'gradient_boosting']:
        n_est = input("\næ¨¹çš„æ•¸é‡ (é è¨­=200): ").strip()
        if n_est:
            model_params['n_estimators'] = int(n_est)
    
    # åˆå§‹åŒ–è¨“ç·´å™¨
    trainer = FlexibleExoplanetTrainer(model_type, **model_params)
    
    # è¼‰å…¥è³‡æ–™
    data_file = './tois.csv'
    df = trainer.load_data(data_file, label_strategy=label_strategy)
    
    # ç‰¹å¾µå·¥ç¨‹
    X = trainer.engineer_features(df)
    y = df['disposition']
    groups = df['TIC ID'] if 'TIC ID' in df.columns else None
    
    # è¨“ç·´
    trainer.train(X, y, groups)
    
    # å„²å­˜
    model_path = trainer.save()
    
    print("\n" + "="*60)
    print("ğŸ‰ è¨“ç·´å®Œæˆï¼")
    print("="*60)
    print(f"\næ¨¡å‹è³‡è¨Š:")
    print(f"  - æ¨¡å‹é¡å‹: {model_type}")
    print(f"  - è¨“ç·´è³‡æ–™: {len(df):,} ç­†")
    print(f"  - ç‰¹å¾µæ•¸: {len(trainer.selected_features)}")
    print(f"  - é¡åˆ¥æ•¸: {len(trainer.label_encoder.classes_)}")
    print(f"  - æ¨¡å‹æª”æ¡ˆ: {model_path}")


if __name__ == "__main__":
    main()